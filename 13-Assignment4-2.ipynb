{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tw.news.yahoo.com/%E5%8F%B0%E8%82%A1%E6%94%B6%E7%9B%A4%E8%B7%8C20-66%E9%BB%9E-053626963.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://tw.news.yahoo.com/_td-news/api/resource/IndexDataService.getExternalMediaNewsList;count=2000;loadMore=true;mrs=%7B%22size%22%3A%7B%22w%22%3A220%2C%22h%22%3A128%7D%7D;newsTab=finance;start=1;tag=%5B%22yct%3A001000298%22%2C%22yct%3A001000123%22%5D?bkt=news-TW-zh-Hant-TW-def&device=desktop&intl=tw&lang=zh-Hant-TW&partner=none&prid=71j7efdch303k&region=TW&site=news&tz=Asia%2FTaipei&ver=2.0.448&returnMeta=true'\n",
    "r = requests.get(url)\n",
    "path = 'https://tw.news.yahoo.com'\n",
    "doc = [(ele['title'], path + ele['url']) for ele in r.json()['data']]\n",
    "index = 2\n",
    "print(doc[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台股收盤跌20.66點\n",
      "（中央社台北6日電）台北股市今天收盤下跌20.66點，為10206.18點，跌幅0.20%，成交金額新台幣858.81億元。1060606\n"
     ]
    }
   ],
   "source": [
    "content = requests.get(doc[index][1]).content\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "one_site_content = ''\n",
    "for p in soup.findAll('p'):\n",
    "    one_site_content += p.getText()\n",
    "print(doc[index][0] + '\\n' + one_site_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_doc = str(2000)\n",
    "finance_url = \"https://tw.news.yahoo.com/_td-news/api/resource/IndexDataService.getExternalMediaNewsList;count=\" + n_doc + \";loadMore=true;mrs=%7B%22size%22%3A%7B%22w%22%3A220%2C%22h%22%3A128%7D%7D;newsTab=finance;start=1;tag=%5B%22yct%3A001000298%22%2C%22yct%3A001000123%22%5D?bkt=news-TW-zh-Hant-TW-def&device=desktop&intl=tw&lang=zh-Hant-TW&partner=none&prid=71j7efdch303k&region=TW&site=news&tz=Asia%2FTaipei&ver=2.0.448&returnMeta=true\"\n",
    "entertain_url = \"https://tw.news.yahoo.com/_td-news/api/resource/IndexDataService.getExternalMediaNewsList;count=\" + n_doc + \";loadMore=true;mrs=%7B%22size%22%3A%7B%22w%22%3A220%2C%22h%22%3A128%7D%7D;newsTab=entertainment;start=1;tag=%5B%22yct%3A001000031%22%5D?bkt=news-TW-zh-Hant-TW-def&device=desktop&intl=tw&lang=zh-Hant-TW&partner=none&prid=btsbko9ch32h7&region=TW&site=news&tz=Asia%2FTaipei&ver=2.0.450&returnMeta=true\"\n",
    "tech_url = \"https://tw.news.yahoo.com/_td-news/api/resource/IndexDataService.getExternalMediaNewsList;count=\" + n_doc + \";loadMore=true;mrs=%7B%22size%22%3A%7B%22w%22%3A220%2C%22h%22%3A128%7D%7D;newsTab=technology;start=1;tag=%5B%22yct%3A001000931%22%2C%22yct%3A001000742%22%2C%22ymedia%3Acategory%3D000000175%22%5D?bkt=news-TW-zh-Hant-TW-def&device=desktop&intl=tw&lang=zh-Hant-TW&partner=none&prid=7p4okcdch32lf&region=TW&site=news&tz=Asia%2FTaipei&ver=2.0.450&returnMeta=true\"\n",
    "def parse_yahoo(url_, cat):\n",
    "    doc_list = []\n",
    "    url = url_\n",
    "    r = requests.get(url)\n",
    "    path = 'https://tw.news.yahoo.com'\n",
    "    doc = [(elem['title'], path + elem['url']) for elem in r.json()['data']]\n",
    "    for index in range(int(n_doc)):\n",
    "        try:\n",
    "            content = requests.get(doc[index][1]).content\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "            one_site_content = ''\n",
    "            for p in soup.findAll('p'):\n",
    "                one_site_content += p.getText()\n",
    "            content = doc[index][0] + '\\n' + one_site_content\n",
    "            doc_list.append(content)\n",
    "            file_name = str(index)\n",
    "            f = open('C:\\\\Users\\\\Aditya Utama Wijaya\\\\Documents\\\\Scripts\\\\python\\\\text mining\\\\13-yahoonews_test\\\\train\\\\' + cat + '\\\\' + file_name, 'w+', encoding = 'utf8')\n",
    "            f.write(content)\n",
    "            f.close()\n",
    "        except:\n",
    "            continue\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fin = parse_yahoo(finance_url, 'finance')\n",
    "ent = parse_yahoo(entertain_url, 'entertainment')\n",
    "tech = parse_yahoo(tech_url, 'technology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "國人退休快樂指數 僅64.3分\n"
     ]
    }
   ],
   "source": [
    "# make sure the news we parse are different from the test set\n",
    "# Load file\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "fin_test_folder = 'C:\\\\Users\\\\Aditya Utama Wijaya\\\\Documents\\\\Scripts\\\\python\\\\text mining\\\\13-yahoonews_test\\\\finance'\n",
    "ent_test_folder = 'C:\\\\Users\\\\Aditya Utama Wijaya\\\\Documents\\\\Scripts\\\\python\\\\text mining\\\\13-yahoonews_test\\\\entertainment'\n",
    "tech_test_folder = 'C:\\\\Users\\\\Aditya Utama Wijaya\\\\Documents\\\\Scripts\\\\python\\\\text mining\\\\13-yahoonews_test\\\\technology'\n",
    "\n",
    "fin_test_file = [join(fin_test_folder, f) for f in listdir(fin_test_folder) if isfile(join(fin_test_folder, f))]\n",
    "ent_test_file = [join(ent_test_folder, f) for f in listdir(ent_test_folder) if isfile(join(ent_test_folder, f))]\n",
    "tech_test_file = [join(tech_test_folder, f) for f in listdir(tech_test_folder) if isfile(join(tech_test_folder, f))]\n",
    "\n",
    "fin_test_title = [open(path, encoding = 'utf8').read().split('\\n')[0] for path in fin_test_file]\n",
    "ent_test_title = [open(path, encoding = 'utf8').read().split('\\n')[0] for path in ent_test_file]\n",
    "tech_test_title = [open(path, encoding = 'utf8').read().split('\\n')[0] for path in tech_test_file]\n",
    "\n",
    "fin_test = [open(path, encoding = 'utf8').read() for path in fin_test_file]\n",
    "ent_test = [open(path, encoding = 'utf8').read() for path in ent_test_file]\n",
    "tech_test = [open(path, encoding = 'utf8').read() for path in tech_test_file]\n",
    "\n",
    "print(fin_test_title[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set:\n",
      "finance: 1957\n",
      "entertainment 1943\n",
      "technology 1884\n",
      "\n",
      "test set:\n",
      "finance: 500\n",
      "entertainment 500\n",
      "technology 500\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "fin_train_folder = 'C:/Users/Aditya Utama Wijaya/Documents/Scripts/python/text mining/13-yahoonews_test/train/finance'\n",
    "ent_train_folder = 'C:/Users/Aditya Utama Wijaya/Documents/Scripts/python/text mining/13-yahoonews_test/train/entertainment'\n",
    "tech_train_folder = 'C:/Users/Aditya Utama Wijaya/Documents/Scripts/python/text mining/13-yahoonews_test/train/technology'\n",
    "\n",
    "fin_train_file = [join(fin_train_folder, f) for f in listdir(fin_train_folder) if isfile(join(fin_train_folder, f))]\n",
    "ent_train_file = [join(ent_train_folder, f) for f in listdir(ent_train_folder) if isfile(join(ent_train_folder, f))]\n",
    "tech_train_file = [join(tech_train_folder, f) for f in listdir(tech_train_folder) if isfile(join(tech_train_folder, f))]\n",
    "\n",
    "fin = [open(path, encoding = 'utf8').read() for path in fin_train_file]\n",
    "ent = [open(path, encoding = 'utf8').read() for path in ent_train_file]\n",
    "tech = [open(path, encoding = 'utf8').read() for path in tech_train_file]\n",
    "\n",
    "# check whether they have same title\n",
    "fin_train = [news for news in fin if news.split('\\n')[0] not in fin_test_title]\n",
    "ent_train = [news for news in ent if news.split('\\n')[0] not in ent_test_title]\n",
    "tech_train = [news for news in tech if news.split('\\n')[0] not in tech_test_title]\n",
    "\n",
    "print('training set:')\n",
    "print('finance:', len(fin_train))\n",
    "print('entertainment', len(ent_train))\n",
    "print('technology', len(tech_train))\n",
    "print()\n",
    "print('test set:')\n",
    "print('finance:', len(fin_test))\n",
    "print('entertainment', len(ent_test))\n",
    "print('technology', len(tech_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "# customize dictionary\n",
    "jieba.set_dictionary('C:\\\\Users\\\\Aditya Utama Wijaya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\Lib\\\\site-packages\\\\jieba\\\\dict.txt.big.txt')\n",
    "jieba.analyse.set_stop_words('C:\\\\Users\\\\Aditya Utama Wijaya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\Lib\\\\site-packages\\\\jieba\\\\stop_words.txt')\n",
    "jieba.analyse.set_idf_path('C:\\\\Users\\\\Aditya Utama Wijaya\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\Lib\\\\site-packages\\\\jieba\\\\idf.txt.big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\Aditya Utama Wijaya\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\site-packages\\jieba\\dict.txt.big.txt ...\n",
      "Dumping model to file cache C:\\Users\\ADITYA~1\\AppData\\Local\\Temp\\jieba.uee653d17f7c7efa808646da2f4c48ec9.cache\n",
      "Loading model cost 4.834 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "train_y = []\n",
    "train_x_wordList = []\n",
    "for cat, cat_num in zip([fin_train, ent_train, tech_train], [1, 2, 3]):\n",
    "    for doc in cat:\n",
    "        word_list = ' '.join(jieba.cut(doc.replace('\\t', '').replace('\\n', '').replace('\\t\\n', '').replace(' ', '')))\n",
    "        train_x_wordList.append(word_list)\n",
    "        train_y.append(cat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 1500 5784 5784\n"
     ]
    }
   ],
   "source": [
    "test_y = []\n",
    "test_x_wordList = []\n",
    "for cat, cat_num in zip([fin_test, ent_test, tech_test], [1, 2, 3]):\n",
    "    for doc in cat:\n",
    "        word_list = ' '.join(jieba.cut(doc.replace('\\t', '').replace('\\n', '').replace('\\t\\n', '').replace(' ', '')))\n",
    "        test_x_wordList.append(word_list)\n",
    "        test_y.append(cat_num)\n",
    "print(len(test_x_wordList), len(test_y), len(train_x_wordList), len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "                    ])\n",
    "text_clf.fit(train_x_wordList, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "      finance       0.84      0.89      0.86       500\n",
      "entertainment       0.96      0.93      0.95       500\n",
      "   technology       0.90      0.87      0.89       500\n",
      "\n",
      "  avg / total       0.90      0.90      0.90      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(test_x_wordList)\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test_y, predicted, target_names = ['finance', 'entertainment', 'technology']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using grid search to find suitable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 1.0, 'vect__ngram_range': (1, 2), 'vect__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vect__use_idf': (True, False),\n",
    "    'clf__C': (10, 1.0, 0.1, 1e-2, 1e-3)\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs = -1)\n",
    "gs_clf = gs_clf.fit(train_x_wordList, train_y)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "      finance       0.82      0.90      0.86       500\n",
      "entertainment       0.96      0.93      0.95       500\n",
      "   technology       0.91      0.85      0.88       500\n",
      "\n",
      "  avg / total       0.90      0.90      0.90      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = gs_clf.predict(test_x_wordList)\n",
    "print(metrics.classification_report(test_y, predicted, target_names = ['finance', 'entertainment', 'technology']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
